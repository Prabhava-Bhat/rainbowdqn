import gymnasium as gym
import numpy as np
from collections import deque, namedtuple
import time
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import math
import pandas as pd
from torch.utils.tensorboard import SummaryWriter

# === Noisy Linear Layer ===
class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features, std_init=0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.register_buffer("weight_epsilon", torch.empty(out_features, in_features))

        self.bias_mu = nn.Parameter(torch.empty(out_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))
        self.register_buffer("bias_epsilon", torch.empty(out_features))

        self.reset_parameters()
        self.reset_noise()

    def reset_parameters(self):
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))

    def reset_noise(self):
        epsilon_in = torch.randn(self.in_features).sign() * torch.randn(self.in_features).abs().sqrt()
        epsilon_out = torch.randn(self.out_features).sign() * torch.randn(self.out_features).abs().sqrt()
        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)

    def forward(self, x):
        if self.training:
            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon
            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon
        else:
            weight = self.weight_mu
            bias = self.bias_mu
        return F.linear(x, weight, bias)

# === Rainbow Network (Dueling + Noisy) ===
class RainbowNetwork(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=256):
        super().__init__()
        self.feature = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU()
        )
        self.value_stream = nn.Sequential(
            NoisyLinear(hidden_size, hidden_size),
            nn.ReLU(),
            NoisyLinear(hidden_size, 1)
        )
        self.advantage_stream = nn.Sequential(
            NoisyLinear(hidden_size, hidden_size),
            nn.ReLU(),
            NoisyLinear(hidden_size, action_size)
        )

    def forward(self, x):
        x = self.feature(x)
        value = self.value_stream(x)
        advantage = self.advantage_stream(x)
        return value + advantage - advantage.mean(dim=1, keepdim=True)

    def reset_noise(self):
        for layer in self.modules():
            if isinstance(layer, NoisyLinear):
                layer.reset_noise()

# === Prioritized Replay Buffer with N-step ===
class PrioritizedReplayBuffer:
    def __init__(self, buffer_size, batch_size, alpha=0.7, n_step=5, gamma=0.99):
        self.capacity = buffer_size
        self.batch_size = batch_size
        self.alpha = alpha
        self.gamma = gamma
        self.memory = []
        self.priorities = np.zeros((buffer_size,), dtype=np.float32)
        self.position = 0
        self.experience = namedtuple("Experience", ["state", "action", "reward", "next_state", "done"])
        self.n_step = n_step
        self.n_step_buffer = deque(maxlen=n_step)

    def _get_n_step_info(self):
        reward, next_state, done = self.n_step_buffer[-1][-3:]
        for _, _, r, n_s, d in reversed(list(self.n_step_buffer)[:-1]):
            reward = r + self.gamma * reward * (1 - d)
            next_state, done = (n_s, d) if d else (next_state, done)
        return reward, next_state, done

    def add(self, state, action, reward, next_state, done):
        self.n_step_buffer.append((state, action, reward, next_state, done))
        if len(self.n_step_buffer) < self.n_step:
            return
        reward, next_state, done = self._get_n_step_info()
        state, action = self.n_step_buffer[0][:2]
        e = self.experience(state, action, reward, next_state, done)
        max_prio = self.priorities.max() if self.memory else 1.0
        if len(self.memory) < self.capacity:
            self.memory.append(e)
        else:
            self.memory[self.position] = e
        self.priorities[self.position] = max_prio
        self.position = (self.position + 1) % self.capacity

    def sample(self, beta=0.5, device="cpu"):
        if len(self.memory) == self.capacity:
            prios = self.priorities
        else:
            prios = self.priorities[:self.position]
        probs = prios ** self.alpha
        probs /= probs.sum()

        indices = np.random.choice(len(self.memory), self.batch_size, p=probs)
        experiences = [self.memory[idx] for idx in indices]

        total = len(self.memory)
        weights = (total * probs[indices]) ** (-beta)
        weights /= weights.max()
        weights = torch.tensor(weights, dtype=torch.float32).unsqueeze(1).to(device)

        batch = self.experience(*zip(*experiences))
        states = torch.tensor(np.vstack(batch.state), dtype=torch.float32).to(device)
        actions = torch.tensor(np.vstack(batch.action), dtype=torch.long).to(device)
        rewards = torch.tensor(np.vstack(batch.reward), dtype=torch.float32).to(device)
        next_states = torch.tensor(np.vstack(batch.next_state), dtype=torch.float32).to(device)
        dones = torch.tensor(np.vstack(batch.done).astype(np.uint8), dtype=torch.float32).to(device)

        return (states, actions, rewards, next_states, dones, indices, weights)

    def update_priorities(self, indices, td_errors):
        for idx, error in zip(indices, td_errors):
            self.priorities[idx] = abs(error) + 1e-5

    def __len__(self):
        return len(self.memory)

# === RainbowAgent ===
class RainbowAgent:
    def __init__(self, state_size, action_size, device,
                 buffer_size=int(1e6), batch_size=128, gamma=0.99,
                 lr=1e-4, tau=5e-3, update_every=4, alpha=0.7, beta_start=0.5,
                 beta_frames=200000, n_step=5):

        self.state_size = state_size
        self.action_size = action_size
        self.device = device
        self.gamma = gamma
        self.tau = tau
        self.lr = lr
        self.update_every = update_every
        self.batch_size = batch_size
        self.beta_start = beta_start
        self.beta_frames = beta_frames
        self.frame = 1
        self.n_step = n_step

        self.policy_net = RainbowNetwork(state_size, action_size).to(device)
        self.target_net = RainbowNetwork(state_size, action_size).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1000, gamma=0.95)

        self.memory = PrioritizedReplayBuffer(buffer_size, batch_size, alpha, n_step, gamma)
        self.t_step = 0

    def step(self, state, action, reward, next_state, done):
        self.memory.add(state, action, reward, next_state, done)
        self.t_step = (self.t_step + 1) % self.update_every
        if self.t_step == 0 and len(self.memory) > self.batch_size:
            beta = min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)
            experiences = self.memory.sample(beta=beta, device=self.device)
            self.learn(experiences)
            self.frame += 1

    def act(self, state):
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        self.policy_net.eval()
        with torch.no_grad():
            action_values = self.policy_net(state)
        self.policy_net.train()
        return np.argmax(action_values.cpu().data.numpy())

    def learn(self, experiences):
        states, actions, rewards, next_states, dones, indices, weights = experiences
        with torch.no_grad():
            next_actions = self.policy_net(next_states).argmax(1, keepdim=True)
            next_q = self.target_net(next_states).gather(1, next_actions)
            target_q = rewards + (self.gamma ** self.n_step) * next_q * (1 - dones)

        expected_q = self.policy_net(states).gather(1, actions)
        td_errors = target_q - expected_q
        loss = (td_errors.pow(2) * weights).mean()

        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1)
        self.optimizer.step()
        self.scheduler.step()

        self.memory.update_priorities(indices, td_errors.detach().cpu().numpy())
        self.soft_update(self.policy_net, self.target_net, self.tau)
        self.policy_net.reset_noise()
        self.target_net.reset_noise()

    def soft_update(self, local_model, target_model, tau):
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)

# === Training Function ===
def train_rainbow(n_episodes=2000, max_t=1000, solve_score=200):
    env = gym.make("LunarLander-v3")
    agent = RainbowAgent(
        state_size=env.observation_space.shape[0],
        action_size=env.action_space.n,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )
    scores = []
    scores_window = deque(maxlen=100)
    start_time = time.time()
    writer = SummaryWriter()

    for i_episode in range(1, n_episodes + 1):
        state, _ = env.reset(seed=i_episode)
        score = 0
        for t in range(max_t):
            action = agent.act(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            agent.step(state, action, reward, next_state, done)
            state = next_state
            score += reward
            if done:
                break
        scores.append(score)
        scores_window.append(score)
        writer.add_scalar("Score", score, i_episode)
        writer.add_scalar("Average100", np.mean(scores_window), i_episode)

        print(f"\rEpisode {i_episode}\tAverage Score: {np.mean(scores_window):.2f}", end="")
        if i_episode % 100 == 0:
            print(f"\rEpisode {i_episode}\tAverage Score: {np.mean(scores_window):.2f}")

        if np.mean(scores_window) >= solve_score:
            print(f"\nEnvironment solved in {i_episode-100} episodes!")
            torch.save(agent.policy_net.state_dict(), 'rainbow_dqn_lunarlander.pth')
            break

    print(f"\nTraining finished in {(time.time() - start_time)/60:.2f} minutes.")
    print(f"Final Average Score: {np.mean(scores_window):.2f}")
    env.close()
    writer.close()

    # Save scores to CSV
    df = pd.DataFrame(scores, columns=['Score'])
    df.to_csv("rainbow_scores.csv", index=False)
    print("Scores saved to 'rainbow_scores.csv'")

    # Save plot
    plt.figure()
    plt.plot(np.arange(len(scores)), scores)
    plt.ylabel("Score")
    plt.xlabel("Episode")
    plt.title("Rainbow DQN on LunarLander-v3")
    plt.grid(True)
    plt.savefig("rainbow_dqn_plot.png")
    plt.show()

    return scores

def record_video(agent, env_name="LunarLander-v3", video_path="videos/", filename="rainbow_dqn_lunarlander.pth"):
    env = gym.make(env_name, render_mode="rgb_array")
    env = gym.wrappers.RecordVideo(env, video_path, episode_trigger=lambda x: True, name_prefix="rainbow_dqn_eval")

    agent.policy_net.load_state_dict(torch.load(filename))
    agent.policy_net.eval()

    state, _ = env.reset(seed=0)
    done = False
    while not done:
        action = agent.act(state)
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

    env.close()
    print(f"Video saved to: {video_path}")

if __name__ == "__main__":
    scores = train_rainbow()
    
    trained_agent = RainbowAgent(
        state_size=8,
        action_size=4,
        device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
    )
    trained_agent.policy_net.load_state_dict(torch.load("rainbow_dqn_lunarlander.pth"))
    record_video(trained_agent)


